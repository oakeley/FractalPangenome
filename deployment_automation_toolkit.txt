# =============================================================================
# Complete Deployment Automation and Cloud Integration Toolkit
# =============================================================================
# This comprehensive toolkit provides everything needed to deploy the
# Fractal Pangenome Database System across multiple cloud providers
# with full CI/CD automation.

# =============================================================================
# CI/CD Pipeline Configuration (.github/workflows/main.yml)
# =============================================================================
---
name: Fractal Pangenome CI/CD Pipeline

on:
  push:
    branches: [main, develop]
    tags: ['v*']
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: fractal-pangenome-db

jobs:
  # =============================================================================
  # Testing Pipeline
  # =============================================================================
  test:
    name: Run Test Suite
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
        test-suite: [unit, integration, performance, security, genomics]
    
    services:
      neo4j:
        image: neo4j:5.15-community
        env:
          NEO4J_AUTH: neo4j/test123
          NEO4J_PLUGINS: '["graph-data-science", "apoc"]'
        ports:
          - 7687:7687
          - 7474:7474
        options: >-
          --health-cmd "curl -f http://localhost:7474 || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y samtools bcftools tabix
    
    - name: Run ${{ matrix.test-suite }} tests
      run: |
        python -m pytest tests/${{ matrix.test-suite }}/ \
          --cov=genome_processor \
          --cov-report=xml \
          --junitxml=test-results-${{ matrix.test-suite }}.xml
      env:
        NEO4J_URI: bolt://localhost:7687
        NEO4J_USER: neo4j
        NEO4J_PASSWORD: test123
        REDIS_URL: redis://localhost:6379
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}-${{ matrix.test-suite }}
        path: |
          test-results-*.xml
          coverage.xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.test-suite == 'unit' && matrix.python-version == '3.11'
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # =============================================================================
  # Security Scanning
  # =============================================================================
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'
    
    - name: Run Bandit security scan
      run: |
        pip install bandit[toml]
        bandit -r genome_processor/ -f json -o bandit-results.json
    
    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-scan-results
        path: |
          trivy-results.sarif
          bandit-results.json

  # =============================================================================
  # Code Quality
  # =============================================================================
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install quality tools
      run: |
        pip install black isort flake8 mypy pylint
    
    - name: Check code formatting with Black
      run: black --check .
    
    - name: Check import sorting with isort
      run: isort --check-only .
    
    - name: Lint with flake8
      run: flake8 .
    
    - name: Type check with mypy
      run: mypy genome_processor/
    
    - name: Lint with pylint
      run: pylint genome_processor/

  # =============================================================================
  # Build and Push Container Images
  # =============================================================================
  build:
    name: Build and Push Images
    runs-on: ubuntu-latest
    needs: [test, security, quality]
    if: github.event_name == 'push'
    
    permissions:
      contents: read
      packages: write
    
    strategy:
      matrix:
        component: [api, worker, frontend, jupyter]
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ github.repository }}-${{ matrix.component }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/${{ matrix.component }}/Dockerfile
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64,linux/arm64

  # =============================================================================
  # Deploy to Staging
  # =============================================================================
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/develop'
    
    environment:
      name: staging
      url: https://staging.fractal-pangenome.example.com
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2
    
    - name: Deploy to EKS
      run: |
        aws eks update-kubeconfig --name fractal-pangenome-staging
        helm upgrade --install fractal-pangenome-staging ./helm/fractal-pangenome \
          --namespace staging \
          --create-namespace \
          --set image.tag=${{ github.sha }} \
          --set environment=staging \
          --values ./helm/values-staging.yaml
    
    - name: Run smoke tests
      run: |
        kubectl wait --for=condition=ready pod -l app=fractal-pangenome-api -n staging --timeout=300s
        kubectl port-forward svc/fractal-pangenome-api 8080:8000 -n staging &
        sleep 10
        curl -f http://localhost:8080/health || exit 1

  # =============================================================================
  # Deploy to Production
  # =============================================================================
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: build
    if: startsWith(github.ref, 'refs/tags/v')
    
    environment:
      name: production
      url: https://fractal-pangenome.example.com
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_PROD }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_PROD }}
        aws-region: us-west-2
    
    - name: Deploy to EKS
      run: |
        aws eks update-kubeconfig --name fractal-pangenome-production
        helm upgrade --install fractal-pangenome ./helm/fractal-pangenome \
          --namespace production \
          --create-namespace \
          --set image.tag=${{ github.ref_name }} \
          --set environment=production \
          --values ./helm/values-production.yaml
    
    - name: Run production health checks
      run: |
        kubectl wait --for=condition=ready pod -l app=fractal-pangenome-api -n production --timeout=600s
        ./scripts/production-health-check.sh

---
# =============================================================================
# Terraform Infrastructure Configuration
# =============================================================================
# terraform/main.tf
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.0"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.0"
    }
  }
  
  backend "s3" {
    bucket = "fractal-pangenome-terraform-state"
    key    = "infrastructure/terraform.tfstate"
    region = "us-west-2"
  }
}

# =============================================================================
# AWS Provider Configuration
# =============================================================================
provider "aws" {
  region = var.aws_region
  
  default_tags {
    tags = {
      Project     = "fractal-pangenome"
      Environment = var.environment
      ManagedBy   = "terraform"
    }
  }
}

# =============================================================================
# Local Variables
# =============================================================================
locals {
  cluster_name = "fractal-pangenome-${var.environment}"
  
  common_tags = {
    Project     = "fractal-pangenome"
    Environment = var.environment
    ManagedBy   = "terraform"
  }
}

# =============================================================================
# VPC Configuration
# =============================================================================
module "vpc" {
  source = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"
  
  name = "${local.cluster_name}-vpc"
  cidr = var.vpc_cidr
  
  azs             = data.aws_availability_zones.available.names
  private_subnets = var.private_subnet_cidrs
  public_subnets  = var.public_subnet_cidrs
  
  enable_nat_gateway   = true
  enable_vpn_gateway   = false
  enable_dns_hostnames = true
  enable_dns_support   = true
  
  public_subnet_tags = {
    "kubernetes.io/cluster/${local.cluster_name}" = "shared"
    "kubernetes.io/role/elb"                      = "1"
  }
  
  private_subnet_tags = {
    "kubernetes.io/cluster/${local.cluster_name}" = "shared"
    "kubernetes.io/role/internal-elb"             = "1"
  }
  
  tags = local.common_tags
}

# =============================================================================
# EKS Cluster
# =============================================================================
module "eks" {
  source = "terraform-aws-modules/eks/aws"
  version = "~> 19.0"
  
  cluster_name    = local.cluster_name
  cluster_version = var.kubernetes_version
  
  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets
  
  cluster_endpoint_public_access  = true
  cluster_endpoint_private_access = true
  
  cluster_addons = {
    coredns = {
      most_recent = true
    }
    kube-proxy = {
      most_recent = true
    }
    vpc-cni = {
      most_recent = true
    }
    aws-ebs-csi-driver = {
      most_recent = true
    }
  }
  
  # Node groups
  eks_managed_node_groups = {
    main = {
      name = "main"
      
      instance_types = var.node_instance_types
      capacity_type  = "ON_DEMAND"
      
      min_size     = var.node_group_min_size
      max_size     = var.node_group_max_size
      desired_size = var.node_group_desired_size
      
      disk_size = 100
      
      labels = {
        role = "main"
      }
      
      taints = []
      
      update_config = {
        max_unavailable_percentage = 25
      }
    }
    
    compute = {
      name = "compute-intensive"
      
      instance_types = ["r5.4xlarge", "r5.8xlarge"]
      capacity_type  = "SPOT"
      
      min_size     = 0
      max_size     = 10
      desired_size = 2
      
      disk_size = 500
      
      labels = {
        role = "compute"
        workload = "genomics"
      }
      
      taints = [
        {
          key    = "compute-intensive"
          value  = "true"
          effect = "NO_SCHEDULE"
        }
      ]
    }
  }
  
  tags = local.common_tags
}

# =============================================================================
# RDS for Metadata
# =============================================================================
module "rds" {
  source = "terraform-aws-modules/rds/aws"
  version = "~> 6.0"
  
  identifier = "${local.cluster_name}-metadata"
  
  engine            = "postgres"
  engine_version    = "15.4"
  instance_class    = var.rds_instance_class
  allocated_storage = var.rds_allocated_storage
  storage_encrypted = true
  
  db_name  = "genomics_metadata"
  username = "genomics_admin"
  password = var.rds_password
  
  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = module.vpc.database_subnet_group
  
  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"
  
  deletion_protection = var.environment == "production"
  
  tags = local.common_tags
}

# =============================================================================
# ElastiCache for Redis
# =============================================================================
resource "aws_elasticache_subnet_group" "redis" {
  name       = "${local.cluster_name}-redis"
  subnet_ids = module.vpc.private_subnets
}

resource "aws_elasticache_replication_group" "redis" {
  replication_group_id       = "${local.cluster_name}-redis"
  description                = "Redis cluster for fractal pangenome caching"
  
  node_type          = var.redis_node_type
  port               = 6379
  parameter_group_name = "default.redis7"
  
  num_cache_clusters = var.redis_num_nodes
  
  subnet_group_name  = aws_elasticache_subnet_group.redis.name
  security_group_ids = [aws_security_group.redis.id]
  
  at_rest_encryption_enabled = true
  transit_encryption_enabled = true
  
  snapshot_retention_limit = 5
  snapshot_window         = "03:00-04:00"
  
  tags = local.common_tags
}

# =============================================================================
# S3 Buckets for Data Storage
# =============================================================================
resource "aws_s3_bucket" "genomic_data" {
  bucket = "${local.cluster_name}-genomic-data"
  
  tags = local.common_tags
}

resource "aws_s3_bucket_encryption_configuration" "genomic_data" {
  bucket = aws_s3_bucket.genomic_data.id
  
  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

resource "aws_s3_bucket_versioning" "genomic_data" {
  bucket = aws_s3_bucket.genomic_data.id
  
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket" "backups" {
  bucket = "${local.cluster_name}-backups"
  
  tags = local.common_tags
}

resource "aws_s3_bucket_lifecycle_configuration" "backups" {
  bucket = aws_s3_bucket.backups.id
  
  rule {
    id     = "backup_lifecycle"
    status = "Enabled"
    
    transition {
      days          = 30
      storage_class = "GLACIER"
    }
    
    transition {
      days          = 90
      storage_class = "DEEP_ARCHIVE"
    }
    
    expiration {
      days = 2555  # 7 years for compliance
    }
  }
}

# =============================================================================
# Security Groups
# =============================================================================
resource "aws_security_group" "rds" {
  name_prefix = "${local.cluster_name}-rds"
  vpc_id      = module.vpc.vpc_id
  
  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr]
  }
  
  tags = local.common_tags
}

resource "aws_security_group" "redis" {
  name_prefix = "${local.cluster_name}-redis"
  vpc_id      = module.vpc.vpc_id
  
  ingress {
    from_port   = 6379
    to_port     = 6379
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr]
  }
  
  tags = local.common_tags
}

# =============================================================================
# Variables
# =============================================================================
variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "us-west-2"
}

variable "environment" {
  description = "Environment (staging/production)"
  type        = string
}

variable "vpc_cidr" {
  description = "VPC CIDR block"
  type        = string
  default     = "10.0.0.0/16"
}

variable "private_subnet_cidrs" {
  description = "Private subnet CIDR blocks"
  type        = list(string)
  default     = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
}

variable "public_subnet_cidrs" {
  description = "Public subnet CIDR blocks"
  type        = list(string)
  default     = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]
}

variable "kubernetes_version" {
  description = "Kubernetes version"
  type        = string
  default     = "1.28"
}

variable "node_instance_types" {
  description = "EC2 instance types for worker nodes"
  type        = list(string)
  default     = ["m5.2xlarge", "m5.4xlarge"]
}

variable "node_group_min_size" {
  description = "Minimum number of nodes"
  type        = number
  default     = 2
}

variable "node_group_max_size" {
  description = "Maximum number of nodes"
  type        = number
  default     = 10
}

variable "node_group_desired_size" {
  description = "Desired number of nodes"
  type        = number
  default     = 3
}

variable "rds_instance_class" {
  description = "RDS instance class"
  type        = string
  default     = "db.r5.2xlarge"
}

variable "rds_allocated_storage" {
  description = "RDS allocated storage in GB"
  type        = number
  default     = 100
}

variable "rds_password" {
  description = "RDS master password"
  type        = string
  sensitive   = true
}

variable "redis_node_type" {
  description = "ElastiCache Redis node type"
  type        = string
  default     = "cache.r5.2xlarge"
}

variable "redis_num_nodes" {
  description = "Number of Redis nodes"
  type        = number
  default     = 3
}

# =============================================================================
# Data Sources
# =============================================================================
data "aws_availability_zones" "available" {
  state = "available"
}

# =============================================================================
# Outputs
# =============================================================================
output "cluster_endpoint" {
  description = "EKS cluster endpoint"
  value       = module.eks.cluster_endpoint
  sensitive   = true
}

output "cluster_name" {
  description = "EKS cluster name"
  value       = module.eks.cluster_name
}

output "rds_endpoint" {
  description = "RDS endpoint"
  value       = module.rds.db_instance_endpoint
  sensitive   = true
}

output "redis_endpoint" {
  description = "Redis endpoint"
  value       = aws_elasticache_replication_group.redis.primary_endpoint_address
  sensitive   = true
}

output "s3_genomic_data_bucket" {
  description = "S3 bucket for genomic data"
  value       = aws_s3_bucket.genomic_data.bucket
}

output "s3_backups_bucket" {
  description = "S3 bucket for backups"
  value       = aws_s3_bucket.backups.bucket
}

---
# =============================================================================
# Helm Chart Configuration (helm/fractal-pangenome/values.yaml)
# =============================================================================
# Default values for fractal-pangenome
global:
  imageRegistry: ghcr.io
  imagePullSecrets: []
  storageClass: "gp3"

replicaCount: 3

image:
  repository: fractal-pangenome-db-api
  pullPolicy: IfNotPresent
  tag: "latest"

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations: {}

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true

service:
  type: ClusterIP
  port: 8000

ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
  hosts:
    - host: api.fractal-pangenome.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: fractal-pangenome-tls
      hosts:
        - api.fractal-pangenome.example.com

resources:
  limits:
    cpu: 2000m
    memory: 4Gi
  requests:
    cpu: 1000m
    memory: 2Gi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

nodeSelector:
  role: main

tolerations: []

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - fractal-pangenome
        topologyKey: kubernetes.io/hostname

# =============================================================================
# Neo4j Configuration
# =============================================================================
neo4j:
  enabled: true
  core:
    numberOfServers: 3
  readReplica:
    numberOfServers: 2
  
  config:
    dbms.memory.heap.initial_size: "8G"
    dbms.memory.heap.max_size: "16G"
    dbms.memory.pagecache.size: "8G"
    dbms.ssl.policy.default.enabled: "true"
    dbms.connector.bolt.tls_level: "REQUIRED"
  
  persistentVolume:
    size: 500Gi
    storageClass: "gp3-ssd"
  
  resources:
    limits:
      cpu: 4000m
      memory: 24Gi
    requests:
      cpu: 2000m
      memory: 16Gi

# =============================================================================
# Redis Configuration
# =============================================================================
redis:
  enabled: true
  architecture: replication
  auth:
    enabled: true
    existingSecret: redis-auth
  
  master:
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    persistence:
      enabled: true
      size: 100Gi
  
  replica:
    replicaCount: 2
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 512Mi

# =============================================================================
# InfluxDB Configuration
# =============================================================================
influxdb:
  enabled: true
  persistence:
    enabled: true
    size: 200Gi
    storageClass: "gp3"
  
  resources:
    limits:
      cpu: 2000m
      memory: 4Gi
    requests:
      cpu: 1000m
      memory: 2Gi
  
  config:
    storage_engine: tsm1
    max_series_per_database: 1000000
    max_values_per_tag: 100000

# =============================================================================
# Monitoring Configuration
# =============================================================================
monitoring:
  prometheus:
    enabled: true
    retention: 90d
    resources:
      limits:
        cpu: 2000m
        memory: 8Gi
      requests:
        cpu: 1000m
        memory: 4Gi
  
  grafana:
    enabled: true
    persistence:
      enabled: true
      size: 10Gi
    
    dashboards:
      genomics: true
      system: true
      database: true

# =============================================================================
# Backup Configuration
# =============================================================================
backup:
  enabled: true
  schedule: "0 2 * * *"  # Daily at 2 AM
  retention: 90
  
  s3:
    bucket: ""  # Set in environment-specific values
    region: us-west-2
    
  encryption:
    enabled: true
    key: ""  # Set via secret

# =============================================================================
# Security Configuration
# =============================================================================
security:
  networkPolicies:
    enabled: true
  
  podSecurityStandards:
    enabled: true
    level: restricted
  
  rbac:
    create: true
  
  encryption:
    atRest: true
    inTransit: true

# =============================================================================
# Application Configuration
# =============================================================================
app:
  config:
    database:
      maxConnections: 100
      connectionTimeout: 30s
    
    api:
      rateLimit: 1000
      timeout: 30s
      cors:
        enabled: true
        origins: []
    
    genomics:
      maxRegionSize: 10000000  # 10Mb
      defaultResolution: auto
      cacheTimeout: 3600

---
# =============================================================================
# Kubernetes Manifests
# =============================================================================
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: fractal-pangenome
  labels:
    name: fractal-pangenome
    istio-injection: enabled
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted

---
# k8s/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: fractal-pangenome-secrets
  namespace: fractal-pangenome
type: Opaque
stringData:
  neo4j-password: "change-me-in-production"
  redis-password: "change-me-in-production"
  jwt-secret: "change-me-in-production"
  encryption-key: "change-me-in-production"

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fractal-pangenome-config
  namespace: fractal-pangenome
data:
  app.yaml: |
    database:
      neo4j:
        uri: "bolt://neo4j:7687"
        max_connections: 100
      redis:
        url: "redis://redis:6379"
        max_connections: 50
      influxdb:
        url: "http://influxdb:8086"
        org: "genomics"
        bucket: "annotations"
    
    api:
      host: "0.0.0.0"
      port: 8000
      workers: 4
      timeout: 30
    
    genomics:
      max_region_size: 10000000
      default_resolution: "auto"
      hilbert_curve_order: 20
    
    security:
      encryption_enabled: true
      audit_logging: true
      rate_limiting: true
    
    monitoring:
      metrics_enabled: true
      health_check_interval: 30
      log_level: "INFO"

---
# k8s/networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: fractal-pangenome-network-policy
  namespace: fractal-pangenome
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: fractal-pangenome
    - namespaceSelector:
        matchLabels:
          name: istio-system
  
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: fractal-pangenome
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
  - to: []
    ports:
    - protocol: TCP
      port: 443

---
# =============================================================================
# Docker Configurations
# =============================================================================
# docker/api/Dockerfile
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    git \
    libhdf5-dev \
    libssl-dev \
    zlib1g-dev \
    libbz2-dev \
    liblzma-dev \
    libcurl4-openssl-dev \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN useradd -m -u 1000 genomics

# Set working directory
WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Set ownership
RUN chown -R genomics:genomics /app

# Switch to non-root user
USER genomics

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# Expose port
EXPOSE 8000

# Run application
CMD ["uvicorn", "rest_api_server:app", "--host", "0.0.0.0", "--port", "8000"]

---
# docker/worker/Dockerfile
FROM python:3.11-slim

# Install system dependencies including bioinformatics tools
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    git \
    samtools \
    bcftools \
    tabix \
    libhdf5-dev \
    libssl-dev \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN useradd -m -u 1000 genomics

# Set working directory
WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Install additional genomics packages
RUN pip install --no-cache-dir \
    pysam \
    cyvcf2 \
    pybedtools \
    pyBigWig

# Copy application code
COPY . .

# Set ownership
RUN chown -R genomics:genomics /app

# Switch to non-root user
USER genomics

# Run worker
CMD ["python", "-m", "celery", "worker", "-A", "genome_processor.tasks", "--loglevel=info"]

---
# =============================================================================
# Monitoring and Alerting
# =============================================================================
# monitoring/prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: fractal-pangenome-alerts
  namespace: fractal-pangenome
spec:
  groups:
  - name: fractal-pangenome.rules
    rules:
    - alert: HighMemoryUsage
      expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
      for: 5m
      labels:
        severity: warning
        component: system
      annotations:
        summary: "High memory usage detected"
        description: "Memory usage is above 90% for more than 5 minutes"
    
    - alert: Neo4jDown
      expr: up{job="neo4j"} == 0
      for: 1m
      labels:
        severity: critical
        component: database
      annotations:
        summary: "Neo4j database is down"
        description: "Neo4j database is not responding"
    
    - alert: APIHighLatency
      expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
      for: 2m
      labels:
        severity: warning
        component: api
      annotations:
        summary: "API response time is high"
        description: "95th percentile response time is above 2 seconds"
    
    - alert: GenomicsQueryFailures
      expr: rate(genomics_query_failures_total[5m]) > 0.1
      for: 2m
      labels:
        severity: critical
        component: genomics
      annotations:
        summary: "High rate of genomics query failures"
        description: "More than 10% of genomics queries are failing"

---
# =============================================================================
# Scripts and Automation
# =============================================================================
# scripts/deploy.sh
#!/bin/bash
set -euo pipefail

ENVIRONMENT=${1:-staging}
NAMESPACE="fractal-pangenome-${ENVIRONMENT}"

echo "üöÄ Deploying Fractal Pangenome Database to ${ENVIRONMENT}"

# Validate environment
if [[ ! "${ENVIRONMENT}" =~ ^(staging|production)$ ]]; then
    echo "‚ùå Invalid environment. Use 'staging' or 'production'"
    exit 1
fi

# Check prerequisites
echo "üìã Checking prerequisites..."
command -v kubectl >/dev/null 2>&1 || { echo "‚ùå kubectl is required"; exit 1; }
command -v helm >/dev/null 2>&1 || { echo "‚ùå helm is required"; exit 1; }

# Apply Kubernetes manifests
echo "üì¶ Applying Kubernetes manifests..."
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/secrets.yaml -n ${NAMESPACE}
kubectl apply -f k8s/configmap.yaml -n ${NAMESPACE}
kubectl apply -f k8s/networkpolicy.yaml -n ${NAMESPACE}

# Deploy with Helm
echo "üéØ Deploying with Helm..."
helm upgrade --install fractal-pangenome-${ENVIRONMENT} ./helm/fractal-pangenome \
    --namespace ${NAMESPACE} \
    --create-namespace \
    --values ./helm/values-${ENVIRONMENT}.yaml \
    --wait \
    --timeout 10m

# Wait for rollout
echo "‚è≥ Waiting for deployment to complete..."
kubectl rollout status deployment/fractal-pangenome-api -n ${NAMESPACE} --timeout=600s

# Run health checks
echo "üè• Running health checks..."
kubectl wait --for=condition=ready pod -l app=fractal-pangenome-api -n ${NAMESPACE} --timeout=300s

# Test endpoint
echo "üß™ Testing API endpoint..."
kubectl port-forward svc/fractal-pangenome-api 8080:8000 -n ${NAMESPACE} &
PORT_FORWARD_PID=$!
sleep 5

if curl -f http://localhost:8080/health >/dev/null 2>&1; then
    echo "‚úÖ Health check passed!"
else
    echo "‚ùå Health check failed!"
    kill $PORT_FORWARD_PID
    exit 1
fi

kill $PORT_FORWARD_PID

echo "üéâ Deployment completed successfully!"
echo "üìä Access Grafana: kubectl port-forward svc/grafana 3000:3000 -n ${NAMESPACE}"
echo "üîç Access Neo4j: kubectl port-forward svc/neo4j 7474:7474 -n ${NAMESPACE}"

---
# scripts/backup.sh
#!/bin/bash
set -euo pipefail

NAMESPACE=${1:-fractal-pangenome-production}
BACKUP_NAME="backup-$(date +%Y%m%d-%H%M%S)"
S3_BUCKET=${2:-fractal-pangenome-backups}

echo "üíæ Starting backup: ${BACKUP_NAME}"

# Create backup job
kubectl create job ${BACKUP_NAME} -n ${NAMESPACE} --image=fractal-pangenome-backup:latest -- \
    /scripts/perform-backup.sh ${BACKUP_NAME} ${S3_BUCKET}

# Wait for completion
kubectl wait --for=condition=complete job/${BACKUP_NAME} -n ${NAMESPACE} --timeout=3600s

# Check status
if kubectl get job ${BACKUP_NAME} -n ${NAMESPACE} -o jsonpath='{.status.succeeded}' | grep -q "1"; then
    echo "‚úÖ Backup completed successfully: ${BACKUP_NAME}"
else
    echo "‚ùå Backup failed"
    kubectl logs job/${BACKUP_NAME} -n ${NAMESPACE}
    exit 1
fi

# Clean up
kubectl delete job ${BACKUP_NAME} -n ${NAMESPACE}

echo "üéâ Backup ${BACKUP_NAME} completed and uploaded to s3://${S3_BUCKET}/"